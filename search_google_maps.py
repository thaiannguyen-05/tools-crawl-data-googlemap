"""
Google Maps Business Scraper
T·ª± ƒë·ªông t√¨m ki·∫øm v√† thu th·∫≠p th√¥ng tin doanh nghi·ªáp t·ª´ Google Maps
Features:
- Multi-tab parallel processing
- Graceful shutdown with Ctrl+C (saves progress)
- Resume from last position (cursor-like)
- Query-based file naming (e.g. "b·∫•t ƒë·ªông s·∫£n" ‚Üí "batdongsan")
- Excel export
"""

import json
import asyncio
import re
import random
import signal
import os
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
from dataclasses import dataclass, field, asdict
from playwright.async_api import async_playwright, Page, BrowserContext, TimeoutError as PlaywrightTimeoutError

# For Excel export
try:
    from openpyxl import Workbook
    from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
    OPENPYXL_AVAILABLE = True
except ImportError:
    OPENPYXL_AVAILABLE = False
    print("‚ö†Ô∏è openpyxl not installed. Run: pip install openpyxl")

# For Vietnamese character removal
try:
    from unidecode import unidecode
    UNIDECODE_AVAILABLE = True
except ImportError:
    UNIDECODE_AVAILABLE = False
    # Fallback mapping for common Vietnamese characters
    VIETNAMESE_MAP = {
        '√°': 'a', '√†': 'a', '·∫£': 'a', '√£': 'a', '·∫°': 'a',
        'ƒÉ': 'a', '·∫Ø': 'a', '·∫±': 'a', '·∫≥': 'a', '·∫µ': 'a', '·∫∑': 'a',
        '√¢': 'a', '·∫•': 'a', '·∫ß': 'a', '·∫©': 'a', '·∫´': 'a', '·∫≠': 'a',
        '√©': 'e', '√®': 'e', '·∫ª': 'e', '·∫Ω': 'e', '·∫π': 'e',
        '√™': 'e', '·∫ø': 'e', '·ªÅ': 'e', '·ªÉ': 'e', '·ªÖ': 'e', '·ªá': 'e',
        '√≠': 'i', '√¨': 'i', '·ªâ': 'i', 'ƒ©': 'i', '·ªã': 'i',
        '√≥': 'o', '√≤': 'o', '·ªè': 'o', '√µ': 'o', '·ªç': 'o',
        '√¥': 'o', '·ªë': 'o', '·ªì': 'o', '·ªï': 'o', '·ªó': 'o', '·ªô': 'o',
        '∆°': 'o', '·ªõ': 'o', '·ªù': 'o', '·ªü': 'o', '·ª°': 'o', '·ª£': 'o',
        '√∫': 'u', '√π': 'u', '·ªß': 'u', '≈©': 'u', '·ª•': 'u',
        '∆∞': 'u', '·ª©': 'u', '·ª´': 'u', '·ª≠': 'u', '·ªØ': 'u', '·ª±': 'u',
        '√Ω': 'y', '·ª≥': 'y', '·ª∑': 'y', '·ªπ': 'y', '·ªµ': 'y',
        'ƒë': 'd',
        '√Å': 'A', '√Ä': 'A', '·∫¢': 'A', '√É': 'A', '·∫†': 'A',
        'ƒÇ': 'A', '·∫Æ': 'A', '·∫∞': 'A', '·∫≤': 'A', '·∫¥': 'A', '·∫∂': 'A',
        '√Ç': 'A', '·∫§': 'A', '·∫¶': 'A', '·∫®': 'A', '·∫™': 'A', '·∫¨': 'A',
        '√â': 'E', '√à': 'E', '·∫∫': 'E', '·∫º': 'E', '·∫∏': 'E',
        '√ä': 'E', '·∫æ': 'E', '·ªÄ': 'E', '·ªÇ': 'E', '·ªÑ': 'E', '·ªÜ': 'E',
        '√ç': 'I', '√å': 'I', '·ªà': 'I', 'ƒ®': 'I', '·ªä': 'I',
        '√ì': 'O', '√í': 'O', '·ªé': 'O', '√ï': 'O', '·ªå': 'O',
        '√î': 'O', '·ªê': 'O', '·ªí': 'O', '·ªî': 'O', '·ªñ': 'O', '·ªò': 'O',
        '∆†': 'O', '·ªö': 'O', '·ªú': 'O', '·ªû': 'O', '·ª†': 'O', '·ª¢': 'O',
        '√ö': 'U', '√ô': 'U', '·ª¶': 'U', '≈®': 'U', '·ª§': 'U',
        '∆Ø': 'U', '·ª®': 'U', '·ª™': 'U', '·ª¨': 'U', '·ªÆ': 'U', '·ª∞': 'U',
        '√ù': 'Y', '·ª≤': 'Y', '·ª∂': 'Y', '·ª∏': 'Y', '·ª¥': 'Y',
        'ƒê': 'D'
    }


# ===== CONFIGURATION =====
STATE_DIR = Path("crawl_state")
OUTPUT_DIR = Path("output")

# Global flags for control
shutdown_requested = False
pause_requested = False
save_requested = False


class KeyboardController:
    """
    Non-blocking keyboard listener for interactive terminal control.
    Supports: P (pause/resume), S (save), Q (quit), H (help)
    """
    
    def __init__(self):
        self.running = False
        self.thread: Optional[asyncio.Task] = None
        self._old_settings = None
        
    def _get_char_non_blocking(self) -> Optional[str]:
        """Get a character from stdin without blocking (Unix only)."""
        import sys
        import select
        
        # Check if there's input available
        if select.select([sys.stdin], [], [], 0)[0]:
            try:
                import termios
                import tty
                
                fd = sys.stdin.fileno()
                old_settings = termios.tcgetattr(fd)
                try:
                    tty.setraw(fd)
                    ch = sys.stdin.read(1)
                finally:
                    termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)
                return ch
            except (ImportError, termios.error):
                return sys.stdin.read(1)
        return None
    
    async def listen(self) -> None:
        """Listen for keyboard input in async loop."""
        global shutdown_requested, pause_requested, save_requested
        
        self.running = True
        
        while self.running:
            try:
                char = self._get_char_non_blocking()
                if char:
                    char_lower = char.lower()
                    
                    if char_lower == 'p':
                        pause_requested = not pause_requested
                        if pause_requested:
                            print("\n   ‚è∏Ô∏è  PAUSED - Nh·∫•n [P] ƒë·ªÉ ti·∫øp t·ª•c...")
                        else:
                            print("\n   ‚ñ∂Ô∏è  RESUMED - Ti·∫øp t·ª•c crawl...")
                    
                    elif char_lower == 's':
                        save_requested = True
                        print("\n   üíæ Save requested...")
                    
                    elif char_lower == 'q':
                        shutdown_requested = True
                        print("\n   üõë Quit requested - ƒêang l∆∞u v√† tho√°t...")
                        break
                    
                    elif char_lower == 'h':
                        self.print_help()
                
                await asyncio.sleep(0.1)  # Check every 100ms
                
            except Exception:
                await asyncio.sleep(0.5)
    
    def print_help(self) -> None:
        """Print help menu."""
        print("\n" + "=" * 50)
        print("   ‚å®Ô∏è  PH√çM T·∫ÆT ƒêI·ªÄU KHI·ªÇN")
        print("=" * 50)
        print("   [P] - Pause/Resume crawl")
        print("   [S] - Save state ngay l·∫≠p t·ª©c")
        print("   [Q] - Quit v√† l∆∞u d·ªØ li·ªáu")
        print("   [H] - Hi·ªán menu n√†y")
        print("=" * 50 + "\n")
    
    def start(self, loop: asyncio.AbstractEventLoop) -> None:
        """Start the keyboard listener."""
        self.thread = loop.create_task(self.listen())
    
    def stop(self) -> None:
        """Stop the keyboard listener."""
        self.running = False
        if self.thread:
            self.thread.cancel()


def print_controls_banner() -> None:
    """Print the keyboard controls banner."""
    print("\n" + "‚îÄ" * 60)
    print("   ‚å®Ô∏è  PH√çM T·∫ÆT: [P]ause  [S]ave  [Q]uit  [H]elp")
    print("‚îÄ" * 60 + "\n")


def sanitize_query_to_filename(query: str) -> str:
    """
    Convert a query string to a valid filename.
    e.g., "b·∫•t ƒë·ªông s·∫£n H√† N·ªôi" -> "batdongsan_ha_noi"
    
    Args:
        query: The search query string
        
    Returns:
        A sanitized filename-safe string
    """
    # First, convert Vietnamese characters to ASCII
    if UNIDECODE_AVAILABLE:
        ascii_text = unidecode(query)
    else:
        # Fallback: use manual mapping
        ascii_text = query
        for viet_char, ascii_char in VIETNAMESE_MAP.items():
            ascii_text = ascii_text.replace(viet_char, ascii_char)
    
    # Convert to lowercase
    ascii_text = ascii_text.lower()
    
    # Replace spaces and special chars with underscore
    ascii_text = re.sub(r'[^a-z0-9]+', '_', ascii_text)
    
    # Remove leading/trailing underscores
    ascii_text = ascii_text.strip('_')
    
    # Collapse multiple underscores
    ascii_text = re.sub(r'_+', '_', ascii_text)
    
    return ascii_text or "query"


@dataclass
class CrawlState:
    """Manages the crawl state for resume functionality."""
    query: str
    filename: str
    urls: List[str] = field(default_factory=list)
    current_index: int = 0
    results: List[Dict[str, str]] = field(default_factory=list)
    last_updated: str = ""
    completed: bool = False
    
    def save(self) -> None:
        """Save current state to JSON file."""
        STATE_DIR.mkdir(exist_ok=True)
        state_file = STATE_DIR / f"{self.filename}_state.json"
        
        self.last_updated = datetime.now().isoformat()
        
        with open(state_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(self), f, ensure_ascii=False, indent=2)
        
        print(f"   üíæ State saved: {state_file}")
    
    @classmethod
    def load(cls, filename: str) -> Optional['CrawlState']:
        """Load state from JSON file if exists."""
        state_file = STATE_DIR / f"{filename}_state.json"
        
        if not state_file.exists():
            return None
        
        try:
            with open(state_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            return cls(
                query=data['query'],
                filename=data['filename'],
                urls=data.get('urls', []),
                current_index=data.get('current_index', 0),
                results=data.get('results', []),
                last_updated=data.get('last_updated', ''),
                completed=data.get('completed', False)
            )
        except (json.JSONDecodeError, KeyError) as e:
            print(f"   ‚ö†Ô∏è Error loading state: {e}")
            return None
    
    @classmethod
    def find_existing(cls, query: str) -> Optional['CrawlState']:
        """Find existing state for a query."""
        filename = sanitize_query_to_filename(query)
        return cls.load(filename)
    
    def mark_completed(self) -> None:
        """Mark this crawl as completed."""
        self.completed = True
        self.save()
    
    def delete_state_file(self) -> None:
        """Delete the state file after successful completion."""
        state_file = STATE_DIR / f"{self.filename}_state.json"
        if state_file.exists():
            state_file.unlink()
            print(f"   üóëÔ∏è State file deleted: {state_file}")


def list_saved_states() -> List[Path]:
    """List all saved state files."""
    if not STATE_DIR.exists():
        return []
    return list(STATE_DIR.glob("*_state.json"))


def export_from_state_files() -> None:
    """
    Export Excel files from all saved state files.
    Useful when crawl was interrupted and Excel wasn't exported.
    """
    state_files = list_saved_states()
    
    if not state_files:
        print("üìÇ Kh√¥ng t√¨m th·∫•y state files trong crawl_state/")
        return
    
    print(f"\nüìÇ T√¨m th·∫•y {len(state_files)} state files:")
    for i, sf in enumerate(state_files, 1):
        print(f"   {i}. {sf.name}")
    
    print()
    
    for state_file in state_files:
        filename = state_file.stem.replace("_state", "")
        state = CrawlState.load(filename)
        
        if state and state.results:
            print(f"\nüìä Exporting {state.filename}: {len(state.results)} results")
            excel_path = save_to_excel(state.results, state.query)
            if excel_path:
                print(f"   ‚úÖ Exported: {excel_path}")
        else:
            print(f"\n‚ö†Ô∏è {filename}: Kh√¥ng c√≥ k·∫øt qu·∫£ ƒë·ªÉ export")


def save_to_excel(
    results: List[Dict[str, str]],
    query: str,
    output_dir: Path = OUTPUT_DIR,
    include_query_col: bool = False,
) -> Optional[Path]:
    """
    Save crawl results to an Excel file.
    
    Args:
        results: List of business info dictionaries
        query: The search query (used for filename)
        output_dir: Output directory path
        
    Returns:
        Path to the created Excel file, or None if failed
    """
    if not OPENPYXL_AVAILABLE:
        print("   ‚ùå openpyxl not available. Cannot export to Excel.")
        print("   üí° Run: pip install openpyxl")
        return None
    
    if not results:
        print("   ‚ö†Ô∏è No results to export")
        return None
    
    # Create output directory
    output_dir.mkdir(exist_ok=True)
    
    # Generate filename from query
    filename = sanitize_query_to_filename(query)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    excel_path = output_dir / f"{filename}_{timestamp}.xlsx"
    
    # Create workbook
    wb = Workbook()
    ws = wb.active
    ws.title = "Results"
    
    # Define headers
    if include_query_col:
        headers = ["STT", "Query", "T√™n", "ƒêi·ªán tho·∫°i", "ƒê·ªãa ch·ªâ", "Website", "Gi·ªù m·ªü c·ª≠a"]
    else:
        headers = ["STT", "T√™n", "ƒêi·ªán tho·∫°i", "ƒê·ªãa ch·ªâ", "Website", "Gi·ªù m·ªü c·ª≠a"]
    
    # Header style
    header_font = Font(bold=True, color="FFFFFF")
    header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
    header_alignment = Alignment(horizontal="center", vertical="center")
    thin_border = Border(
        left=Side(style='thin'),
        right=Side(style='thin'),
        top=Side(style='thin'),
        bottom=Side(style='thin')
    )
    
    # Write headers
    for col, header in enumerate(headers, 1):
        cell = ws.cell(row=1, column=col, value=header)
        cell.font = header_font
        cell.fill = header_fill
        cell.alignment = header_alignment
        cell.border = thin_border
    
    # Write data
    for row, business in enumerate(results, 2):
        ws.cell(row=row, column=1, value=row - 1).border = thin_border

        if include_query_col:
            ws.cell(row=row, column=2, value=business.get('query', '')).border = thin_border
            ws.cell(row=row, column=3, value=business.get('name', '')).border = thin_border
            ws.cell(row=row, column=4, value=business.get('phone', '')).border = thin_border
            ws.cell(row=row, column=5, value=business.get('address', '')).border = thin_border
            ws.cell(row=row, column=6, value=business.get('website', '')).border = thin_border
            ws.cell(row=row, column=7, value=business.get('opening_hours', '')).border = thin_border
        else:
            ws.cell(row=row, column=2, value=business.get('name', '')).border = thin_border
            ws.cell(row=row, column=3, value=business.get('phone', '')).border = thin_border
            ws.cell(row=row, column=4, value=business.get('address', '')).border = thin_border
            ws.cell(row=row, column=5, value=business.get('website', '')).border = thin_border
            ws.cell(row=row, column=6, value=business.get('opening_hours', '')).border = thin_border
    
    # Adjust column widths
    if include_query_col:
        column_widths = [6, 30, 40, 15, 60, 40, 30]
    else:
        column_widths = [6, 40, 15, 60, 40, 30]
    for col, width in enumerate(column_widths, 1):
        ws.column_dimensions[ws.cell(row=1, column=col).column_letter].width = width
    
    # Save workbook
    wb.save(excel_path)
    print(f"   üìä Excel saved: {excel_path}")
    print(f"   üìà Total records: {len(results)}")
    
    return excel_path


def save_combined_excel(
    results_by_query: Dict[str, List[Dict[str, str]]],
    output_dir: Path = OUTPUT_DIR,
) -> Optional[Path]:
    """Save all query results into a single Excel file with a Query column."""
    combined: List[Dict[str, str]] = []

    for query, businesses in results_by_query.items():
        for business in businesses:
            row = dict(business)
            row["query"] = query
            combined.append(row)

    if not combined:
        print("   ‚ö†Ô∏è Kh√¥ng c√≥ k·∫øt qu·∫£ ƒë·ªÉ export (combined)")
        return None
    
    # Reuse save_to_excel with query column
    return save_to_excel(combined, query="combined", output_dir=output_dir, include_query_col=True)


class GoogleMapsScraper:
    """Scraper Google Maps s·ª≠ d·ª•ng Playwright"""
    
    def __init__(self, headless: bool = False, concurrent_tabs: int = 3):
        self.headless = headless
        self.concurrent_tabs = concurrent_tabs
        self.max_scroll_attempts = 100  # S·ªë l·∫ßn scroll t·ªëi ƒëa ƒë·ªÉ load h·∫øt k·∫øt qu·∫£
        self.max_retries = 3  # S·ªë l·∫ßn retry khi timeout
    
    async def search_google_maps(self, query: str, page: Page, context: BrowserContext) -> List[Dict]:
        """
        T√¨m ki·∫øm tr√™n Google Maps v√† l·∫•y danh s√°ch k·∫øt qu·∫£
        
        Args:
            query: T·ª´ kh√≥a t√¨m ki·∫øm
            page: Playwright page instance
            context: Browser context for multi-tab processing
            
        Returns:
            List c√°c k·∫øt qu·∫£ business
        """
        from urllib.parse import quote_plus
        
        encoded_query = quote_plus(query)
        maps_url = f"https://www.google.com/maps/search/{encoded_query}"
        
        try:
            print(f"   üó∫Ô∏è  ƒêang truy c·∫≠p Google Maps...")
            await page.goto(maps_url, wait_until="domcontentloaded", timeout=60000)
            
            # ƒê·ª£i k·∫øt qu·∫£ load v·ªõi smart wait
            print(f"   ‚è≥ ƒêang ch·ªù k·∫øt qu·∫£ Maps load...")
            try:
                await page.wait_for_selector('div[role="feed"]', timeout=10000)
                print(f"   ‚úÖ ƒê√£ load ƒë∆∞·ª£c danh s√°ch k·∫øt qu·∫£")
            except:
                print(f"   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y danh s√°ch k·∫øt qu·∫£")
                return []
            
            # Scroll ƒë·ªÉ load t·∫•t c·∫£ k·∫øt qu·∫£
            results_count = await self._scroll_to_load_all(page)
            print(f"   üìä T·ªïng s·ªë k·∫øt qu·∫£ sau khi scroll: {results_count}")
            
            # Parse t·∫•t c·∫£ k·∫øt qu·∫£ v·ªõi multi-tab
            businesses = await self._parse_all_results_with_tabs(page, context)
            
            return businesses
            
        except PlaywrightTimeoutError:
            print(f"   ‚è±Ô∏è Timeout khi load Google Maps")
            return []
        except Exception as e:
            print(f"   ‚ùå L·ªói: {type(e).__name__}: {e}")
            return []
    
    async def _scroll_to_load_all(self, page: Page) -> int:
        """
        Scroll sidebar ƒë·ªÉ load to√†n b·ªô k·∫øt qu·∫£
        
        Returns:
            S·ªë l∆∞·ª£ng k·∫øt qu·∫£ hi·ªán t·∫°i
        """
        print(f"   üîÑ ƒêang scroll ƒë·ªÉ load th√™m k·∫øt qu·∫£...")
        
        # Selector cho scrollable container
        # Google Maps c√≥ th·ªÉ thay ƒë·ªïi, th·ª≠ nhi·ªÅu selector
        scrollable_selectors = [
            'div[role="feed"]',
            'div.m6QErb',  # Class name c√≥ th·ªÉ thay ƒë·ªïi
            '[aria-label*="Results"]',
        ]
        
        scrollable_elem = None
        for selector in scrollable_selectors:
            elem = await page.query_selector(selector)
            if elem:
                scrollable_elem = elem
                print(f"      ‚úì T√¨m th·∫•y scrollable container: {selector}")
                break
        
        if not scrollable_elem:
            print(f"      ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y scrollable container")
            return 0
        
        try:
            previous_count = 0
            no_change_attempts = 0
            
            for i in range(self.max_scroll_attempts):
                # Scroll xu·ªëng
                await page.evaluate('''
                    const scrollable = document.querySelector('div[role="feed"]');
                    if (scrollable) {
                        scrollable.scrollBy(0, scrollable.scrollHeight);
                    }
                ''')
                
                # ƒê·ª£i load - gi·∫£m t·ª´ 3s xu·ªëng 1.5s
                await asyncio.sleep(1.5)
                
                # ƒê·∫øm s·ªë item
                # Th·ª≠ nhi·ªÅu selector
                items = []
                for sel in ['a[href*="/maps/place/"]', 'div[role="article"]', 'a.hfpxzc']:
                    items = await page.query_selector_all(sel)
                    if items:
                        break
                
                current_count = len(items)
                
                if current_count > previous_count:
                    print(f"      ‚îú‚îÄ Scroll {i+1}: {current_count} k·∫øt qu·∫£ (+{current_count - previous_count})")
                    previous_count = current_count
                    no_change_attempts = 0
                else:
                    no_change_attempts += 1
                    print(f"      ‚îú‚îÄ Scroll {i+1}: {current_count} k·∫øt qu·∫£ (kh√¥ng tƒÉng)")
                    
                    if no_change_attempts >= 3:
                        print(f"      ‚îî‚îÄ ƒê√£ load h·∫øt (kh√¥ng tƒÉng sau 3 l·∫ßn)")
                        break
            
            return previous_count
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è L·ªói scroll: {e}")
            return 0
    
    async def _parse_all_results_with_tabs(self, page: Page, context: BrowserContext) -> List[Dict]:
        """
        Parse t·∫•t c·∫£ k·∫øt qu·∫£ s·ª≠ d·ª•ng multi-tab parallel processing
        
        Returns:
            List c√°c business info
        """
        businesses = []
        
        try:
            # Thu th·∫≠p t·∫•t c·∫£ URLs t·ª´ search results
            possible_selectors = [
                'a.hfpxzc',  # Link ch√≠nh c·ªßa m·ªói business (ph·ªï bi·∫øn nh·∫•t)
                'a[href*="/maps/place/"]',  # Fallback
            ]
            
            urls = []
            used_selector = None
            
            for selector in possible_selectors:
                items = await page.query_selector_all(selector)
                if items and len(items) > 0:
                    used_selector = selector
                    print(f"   ‚úÖ T√¨m th·∫•y {len(items)} items v·ªõi selector: {selector}")
                    
                    # Extract URLs
                    for item in items:
                        href = await item.get_attribute('href')
                        if href and '/maps/place/' in href:
                            urls.append(href)
                    break
            
            if not urls:
                print(f"   ‚ùå Kh√¥ng t√¨m th·∫•y business URLs!")
                # Debug: l∆∞u HTML v√† screenshot
                html_content = await page.content()
                with open('debug_maps.html', 'w', encoding='utf-8') as f:
                    f.write(html_content)
                await page.screenshot(path='debug_maps.png')
                print(f"   üíæ ƒê√£ l∆∞u debug_maps.html v√† debug_maps.png")
                return businesses
            
            # Lo·∫°i b·ªè duplicates
            urls = list(dict.fromkeys(urls))
            
            # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng
            max_items = min(len(urls), 30)
            urls = urls[:max_items]
            
            print(f"   üìù S·∫Ω crawl {len(urls)} businesses v·ªõi {self.concurrent_tabs} tabs song song")
            print(f"   üí° Multi-tab parallel processing...\n")
            
            # Process URLs in batches
            batch_size = self.concurrent_tabs
            total_processed = 0
            
            for batch_idx in range(0, len(urls), batch_size):
                batch_urls = urls[batch_idx:batch_idx + batch_size]
                batch_num = (batch_idx // batch_size) + 1
                total_batches = (len(urls) + batch_size - 1) // batch_size
                
                print(f"   üîÑ Batch {batch_num}/{total_batches}: Processing {len(batch_urls)} items in parallel...")
                
                # Process batch in parallel
                tasks = [
                    self._extract_from_url(url, context, total_processed + i + 1, max_items)
                    for i, url in enumerate(batch_urls)
                ]
                
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # Collect successful results
                for result in batch_results:
                    if isinstance(result, dict) and result.get('name'):
                        businesses.append(result)
                    elif isinstance(result, Exception):
                        print(f"      ‚ö†Ô∏è Error in batch: {result}")
                
                total_processed += len(batch_urls)
                
                # Delay between batches v·ªõi random jitter (anti-detection)
                if batch_idx + batch_size < len(urls):
                    batch_delay = 1.5 + random.uniform(0, 1)
                    await asyncio.sleep(batch_delay)
                
                print()
            
            print(f"   ‚úÖ ƒê√£ parse th√†nh c√¥ng {len(businesses)}/{max_items} k·∫øt qu·∫£")
            return businesses
            
        except Exception as e:
            print(f"   ‚ùå L·ªói khi parse: {e}")
            import traceback
            traceback.print_exc()
            return businesses
    
    async def _extract_from_url(self, url: str, context: BrowserContext, index: int, total: int) -> Optional[Dict]:
        """
        M·ªü URL trong tab m·ªõi v√† extract business info v·ªõi retry logic
        
        Args:
            url: Business detail URL
            context: Browser context
            index: Current index for logging
            total: Total items for logging
            
        Returns:
            Business info dict ho·∫∑c None
        """
        page = None
        
        # Retry with exponential backoff
        for attempt in range(self.max_retries):
            try:
                # M·ªü tab m·ªõi
                page = await context.new_page()
                
                # Stagger tab opening v·ªõi random jitter ƒë·ªÉ tr√°nh b·ªã detect
                base_delay = 0.05 * (index % self.concurrent_tabs)
                jitter = random.uniform(0, 0.1)
                await asyncio.sleep(base_delay + jitter)
                
                # Navigate v·ªõi timeout tƒÉng d·∫ßn theo attempt
                timeout = 30000 * (attempt + 1)
                await page.goto(url, wait_until="domcontentloaded", timeout=timeout)
                
                # Thay v√¨ wait networkidle, wait cho selector quan tr·ªçng
                try:
                    # Wait cho t√™n business xu·∫•t hi·ªán
                    await page.wait_for_selector('h1', timeout=8000)
                except:
                    # N·∫øu kh√¥ng c√≥ h1, v·∫´n th·ª≠ extract
                    pass
                
                # Th√™m delay nh·ªè v·ªõi random jitter ƒë·ªÉ panel load ƒë·∫ßy ƒë·ªß
                await asyncio.sleep(1 + random.uniform(0, 0.3))
                
                # Extract info
                business_info = await self._extract_from_detail_panel(page)
                
                if business_info and business_info.get('name'):
                    print(f"      ‚úì [{index}/{total}] {business_info['name'][:50]}")
                    if business_info.get('phone'):
                        print(f"          üìû {business_info['phone']}")
                else:
                    print(f"      ‚ö†Ô∏è [{index}/{total}] Kh√¥ng l·∫•y ƒë∆∞·ª£c th√¥ng tin")
                
                # Success - close page and return
                await page.close()
                return business_info
                
            except PlaywrightTimeoutError as e:
                if page:
                    await page.close()
                    page = None
                
                if attempt < self.max_retries - 1:
                    # Exponential backoff before retry
                    backoff = (2 ** attempt) + random.uniform(0, 1)
                    print(f"      üîÑ [{index}/{total}] Timeout, ƒëang retry sau {backoff:.1f}s...")
                    await asyncio.sleep(backoff)
                else:
                    print(f"      ‚ùå [{index}/{total}] L·ªói: Timeout sau {self.max_retries} l·∫ßn th·ª≠")
                    return None
                    
            except Exception as e:
                if page:
                    await page.close()
                    page = None
                    
                print(f"      ‚ùå [{index}/{total}] L·ªói: {type(e).__name__}: {str(e)[:50]}")
                return None
        
        return None
    
    async def _extract_from_detail_panel(self, page: Page) -> Optional[Dict]:
        """
        Extract th√¥ng tin t·ª´ detail panel b√™n ph·∫£i
        (Sau khi ƒë√£ click v√†o m·ªôt business)
        """
        try:
            
            # L·∫•y t√™n - nhi·ªÅu selector kh√°c nhau
            name = None
            name_selectors = [
                'h1.DUwDvf',  # Selector ph·ªï bi·∫øn nh·∫•t
                'h1.fontHeadlineLarge',
                'h1',
                'div.fontHeadlineLarge span',
                '[role="main"] h1',
            ]
            
            for selector in name_selectors:
                name_elem = await page.query_selector(selector)
                if name_elem:
                    name_text = await name_elem.inner_text()
                    name_text = name_text.strip()
                    if name_text and len(name_text) > 2:
                        name = name_text
                        break
            
            if not name:
                return None
            
            # L·∫•y s·ªë ƒëi·ªán tho·∫°i - nhi·ªÅu c√°ch
            phone = None
            
            # C√°ch 1: T√¨m button c√≥ data-item-id ch·ª©a "phone"
            phone_button = await page.query_selector('button[data-item-id*="phone"]')
            if phone_button:
                aria_label = await phone_button.get_attribute('aria-label') or ''
                phone = self._extract_phone(aria_label)
            
            # C√°ch 2: T√¨m link tel:
            if not phone:
                tel_link = await page.query_selector('a[href^="tel:"]')
                if tel_link:
                    href = await tel_link.get_attribute('href') or ''
                    phone = self._extract_phone(href)
            
            # C√°ch 3: T√¨m trong aria-label c√≥ "Phone"
            if not phone:
                phone_buttons = await page.query_selector_all('button[aria-label*="Phone"], button[aria-label*="ƒêi·ªán tho·∫°i"]')
                for btn in phone_buttons:
                    aria_label = await btn.get_attribute('aria-label') or ''
                    phone = self._extract_phone(aria_label)
                    if phone:
                        break
            
            # C√°ch 4: T√¨m trong to√†n b·ªô panel text
            if not phone:
                # L·∫•y text t·ª´ ph·∫ßn th√¥ng tin chi ti·∫øt
                detail_sections = await page.query_selector_all('div.rogA2c')  # Sections ch·ª©a info
                for section in detail_sections:
                    text = await section.inner_text()
                    phone = self._extract_phone(text)
                    if phone:
                        break
            
            # L·∫•y ƒë·ªãa ch·ªâ
            address = "Ch∆∞a c√≥ th√¥ng tin"
            
            # C√°ch 1: T·ª´ button address
            addr_button = await page.query_selector('button[data-item-id*="address"]')
            if addr_button:
                aria_label = await addr_button.get_attribute('aria-label') or ''
                if 'Address:' in aria_label or 'ƒê·ªãa ch·ªâ:' in aria_label:
                    parts = aria_label.replace('Address:', '|').replace('ƒê·ªãa ch·ªâ:', '|').split('|')
                    if len(parts) > 1:
                        address = parts[1].strip()
            
            # C√°ch 2: T√¨m trong div ch·ª©a ƒë·ªãa ch·ªâ (th∆∞·ªùng c√≥ class fontBodyMedium)
            if address == "Ch∆∞a c√≥ th√¥ng tin":
                addr_divs = await page.query_selector_all('div.fontBodyMedium')
                for div in addr_divs:
                    text = await div.inner_text()
                    text = text.strip()
                    # ƒê·ªãa ch·ªâ th∆∞·ªùng c√≥ t√™n th√†nh ph·ªë v√† d√†i h∆°n
                    if any(city in text for city in ['H√† N·ªôi', 'TP.HCM', 'ƒê√† N·∫µng', 'C·∫ßn Th∆°', 'H·∫£i Ph√≤ng', 'Vi·ªát Nam']):
                        if len(text) > 15 and not any(x in text for x in ['‚òÖ', 'ƒë√°nh gi√°', 'rating', 'M·ªü c·ª≠a', 'ƒê√≥ng c·ª≠a']):
                            address = text
                            break
            
            # C√°ch 3: Fallback - t√¨m trong to√†n b·ªô panel
            if address == "Ch∆∞a c√≥ th√¥ng tin":
                panel_elem = await page.query_selector('[role="main"]')
                if panel_elem:
                    panel_text = await panel_elem.inner_text()
                    address = self._extract_address_from_text(panel_text)
            
            # L·∫•y website
            website = None
            
            # C√°ch 1: T√¨m button c√≥ data-item-id ch·ª©a "authority" ho·∫∑c "website"
            website_button = await page.query_selector('button[data-item-id*="authority"], button[data-item-id*="website"]')
            if website_button:
                aria_label = await website_button.get_attribute('aria-label') or ''
                # Extract URL t·ª´ aria-label
                website = self._extract_website(aria_label)
            
            # C√°ch 2: T√¨m link c√≥ href b·∫Øt ƒë·∫ßu b·∫±ng http
            if not website:
                # T√¨m trong panel ch√≠nh, tr√°nh c√°c link internal c·ªßa Google Maps
                panel_elem = await page.query_selector('[role="main"]')
                if panel_elem:
                    website_links = await panel_elem.query_selector_all('a[href^="http"]')
                    for link in website_links:
                        href = await link.get_attribute('href') or ''
                        # Lo·∫°i b·ªè c√°c link c·ªßa Google
                        if 'google.com' not in href and 'gstatic.com' not in href:
                            website = href
                            break
            
            # L·∫•y th·ªùi gian ho·∫°t ƒë·ªông (opening hours)
            opening_hours = None
            
            # C√°ch 1: T√¨m button c√≥ data-item-id ch·ª©a "hours"
            hours_button = await page.query_selector('button[data-item-id*="hours"]')
            if hours_button:
                aria_label = await hours_button.get_attribute('aria-label') or ''
                opening_hours = self._extract_opening_hours(aria_label)
            
            # C√°ch 2: T√¨m trong c√°c div ch·ª©a th√¥ng tin gi·ªù m·ªü c·ª≠a
            if not opening_hours:
                # T√¨m text c√≥ ch·ª©a "Open", "Closes", "M·ªü c·ª≠a", "ƒê√≥ng c·ª≠a"
                hours_indicators = ['Open', 'Closes', 'Opens', 'M·ªü c·ª≠a', 'ƒê√≥ng c·ª≠a', '24 hours', '24 gi·ªù']
                all_divs = await page.query_selector_all('div.fontBodyMedium, div.fontBodySmall')
                for div in all_divs:
                    text = await div.inner_text()
                    text = text.strip()
                    if any(indicator in text for indicator in hours_indicators):
                        # T√¨m th√™m context xung quanh ƒë·ªÉ l·∫•y ƒë·∫ßy ƒë·ªß th√¥ng tin
                        parent = await div.evaluate_handle('el => el.parentElement')
                        if parent:
                            hours_text = await parent.as_element().inner_text()
                            hours_text = hours_text.strip()
                            if len(hours_text) > 3:
                                opening_hours = hours_text
                                break
            
            return {
                "name": name,
                "phone": phone,
                "address": address,
                "website": website,
                "opening_hours": opening_hours,
            }
            
        except Exception as e:
            print(f"         L·ªói extract detail: {e}")
            return None
    
    def _extract_address_from_text(self, text: str) -> str:
        """Extract ƒë·ªãa ch·ªâ t·ª´ m·ªôt ƒëo·∫°n text d√†i"""
        lines = text.split('\n')
        cities = ['H√† N·ªôi', 'TP.HCM', 'TP HCM', 'S√†i G√≤n', 'ƒê√† N·∫µng', 'C·∫ßn Th∆°', 'H·∫£i Ph√≤ng', 'Vi·ªát Nam']
        
        for line in lines:
            line = line.strip()
            # T√¨m d√≤ng ch·ª©a t√™n th√†nh ph·ªë v√† ƒë·ªß d√†i
            for city in cities:
                if city in line and len(line) > 15:
                    # Lo·∫°i b·ªè c√°c prefix kh√¥ng c·∫ßn thi·∫øt
                    if ':' in line:
                        line = line.split(':', 1)[1].strip()
                    return line
        
        return "Ch∆∞a c√≥ th√¥ng tin"
    
    def _extract_website(self, text: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t website URL t·ª´ text"""
        # Pattern cho URL
        url_pattern = r'https?://[^\s\"\',<>]+'
        matches = re.findall(url_pattern, text)
        
        if matches:
            url = matches[0]
            # Lo·∫°i b·ªè c√°c URL c·ªßa Google
            if 'google.com' not in url and 'gstatic.com' not in url:
                return url
        
        # N·∫øu kh√¥ng t√¨m th·∫•y http://, th·ª≠ t√¨m domain pattern
        domain_pattern = r'(?:www\.)?[a-zA-Z0-9-]+\.[a-zA-Z]{2,}(?:\.[a-zA-Z]{2,})?'
        domain_matches = re.findall(domain_pattern, text)
        
        if domain_matches:
            domain = domain_matches[0]
            # Th√™m https:// n·∫øu ch∆∞a c√≥
            if not domain.startswith(('http://', 'https://')):
                return f'https://{domain}'
            return domain
        
        return None
    
    def _extract_opening_hours(self, text: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t th√¥ng tin gi·ªù m·ªü c·ª≠a t·ª´ text"""
        # L√†m s·∫°ch aria-label
        # Th∆∞·ªùng c√≥ format: "Hours: Open ‚ãÖ Closes 5 PM" ho·∫∑c "Gi·ªù: M·ªü c·ª≠a ‚ãÖ ƒê√≥ng c·ª≠a 17:00"
        
        # Lo·∫°i b·ªè c√°c prefix nh∆∞ "Hours:", "Gi·ªù:", etc.
        cleaned = text
        for prefix in ['Hours:', 'Gi·ªù:', 'Opening hours:', 'Th·ªùi gian m·ªü c·ª≠a:']:
            if prefix in cleaned:
                cleaned = cleaned.split(prefix, 1)[1].strip()
        
        # N·∫øu c√≥ n·ªôi dung h·ª£p l·ªá
        if len(cleaned) > 3:
            # L√†m s·∫°ch th√™m c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát
            cleaned = cleaned.replace('‚ãÖ', '‚Ä¢').strip()
            return cleaned
        
        return None
    
    def _extract_phone(self, text: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t s·ªë ƒëi·ªán tho·∫°i t·ª´ text"""
        # Pattern cho s·ªë ƒëi·ªán tho·∫°i Vi·ªát Nam
        patterns = [
            r'(?:\+84|84|0)[\s.-]?\d{1,4}[\s.-]?\d{3}[\s.-]?\d{3,4}',
            r'(?:\+84|84|0)\d{9,10}',
            r'\b\d{10,11}\b',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            if matches:
                # L√†m s·∫°ch
                phone = re.sub(r'[^\d+]', '', matches[0])
                
                # Chu·∫©n h√≥a
                if phone.startswith('+84'):
                    phone = '0' + phone[3:]
                elif phone.startswith('84'):
                    phone = '0' + phone[2:]
                
                if 10 <= len(phone) <= 11:
                    return phone
        
        return None
    
    async def run_searches(self, queries: List[str], delay: float = 3.0) -> Dict[str, List[Dict]]:
        """
        Ch·∫°y nhi·ªÅu query search tr√™n Maps
        
        Args:
            queries: Danh s√°ch query
            delay: Delay gi·ªØa c√°c query
            
        Returns:
            Dict v·ªõi key l√† query, value l√† list k·∫øt qu·∫£
        """
        all_results = {}
        
        async with async_playwright() as p:
            print("üåê ƒêang kh·ªüi ƒë·ªông browser...")
            
            # Launch v·ªõi args t∆∞∆°ng t·ª± batdongsan_final.py
            browser = await p.chromium.launch(
                headless=self.headless,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                ]
            )
            
            # Context options
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080},
                locale="vi-VN",
                timezone_id="Asia/Ho_Chi_Minh",
            )
            
            page = await context.new_page()
            
            try:
                for i, query in enumerate(queries, 1):
                    print(f"\nüîç [{i}/{len(queries)}] ƒêang search: {query}")
                    
                    # Search tr√™n Maps v·ªõi context for multi-tab
                    businesses = await self.search_google_maps(query, page, context)
                    
                    all_results[query] = businesses
                    print(f"   ‚úÖ T·ªïng c·ªông: {len(businesses)} k·∫øt qu·∫£\n")
                    
                    # üíæ Incremental save ƒë·ªÉ tr√°nh m·∫•t data khi crash
                    temp_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    temp_file = f"temp_incremental_{temp_timestamp}.json"
                    
                    try:
                        with open(temp_file, 'w', encoding='utf-8') as f:
                            json.dump(all_results, f, ensure_ascii=False, indent=2)
                        print(f"   üíæ ƒê√£ l∆∞u t·∫°m: {temp_file}")
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è Kh√¥ng th·ªÉ l∆∞u t·∫°m: {e}")
                    
                    # Delay v·ªõi random jitter
                    if i < len(queries):
                        delay_time = delay + random.uniform(0, 2)
                        print(f"   ‚è≥ Ch·ªù {delay_time:.1f}s tr∆∞·ªõc query ti·∫øp theo...")
                        await asyncio.sleep(delay_time)
            
            finally:
                await browser.close()
        
        return all_results


def save_results(results: Dict[str, List[Dict]], output_file: str, timestamp: str = "", chunk_size: int = 1000):
    """L∆∞u k·∫øt qu·∫£ v√†o JSON files v·ªõi timestamp prefix
    T·ª± ƒë·ªông chia th√†nh nhi·ªÅu files n·∫øu > chunk_size records
    
    Args:
        results: K·∫øt qu·∫£ scraping
        output_file: T√™n file g·ªëc
        timestamp: Timestamp ƒë·ªÉ th√™m v√†o prefix (format: YYYYMMDD_HHMMSS)
        chunk_size: S·ªë records t·ªëi ƒëa m·ªói file (default: 1000)
    """
    # G·ªôp v√† lo·∫°i tr√πng
    all_businesses = []
    seen_names = set()
    
    for query, businesses in results.items():
        for business in businesses:
            name = business.get("name", "")
            
            # Lo·∫°i tr√πng theo t√™n
            if name and name not in seen_names:
                seen_names.add(name)
                all_businesses.append(business)
    
    total_records = len(all_businesses)
    
    # T√≠nh s·ªë files c·∫ßn thi·∫øt
    num_files = (total_records + chunk_size - 1) // chunk_size
    
    print(f"\nüíæ T·ªïng c·ªông {total_records} doanh nghi·ªáp")
    
    if num_files == 1:
        # Ch·ªâ 1 file, l∆∞u b√¨nh th∆∞·ªùng
        if timestamp:
            if '.' in output_file:
                name_parts = output_file.rsplit('.', 1)
                final_filename = f"{timestamp}_{name_parts[0]}.{name_parts[1]}"
            else:
                final_filename = f"{timestamp}_{output_file}"
        else:
            final_filename = output_file
        
        with open(final_filename, 'w', encoding='utf-8') as f:
            json.dump(all_businesses, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ ƒê√£ l∆∞u v√†o: {final_filename}")
    else:
        # Nhi·ªÅu files, chia th√†nh chunks
        print(f"üì¶ S·∫Ω chia th√†nh {num_files} files ({chunk_size} records/file)")
        
        for i in range(num_files):
            start_idx = i * chunk_size
            end_idx = min((i + 1) * chunk_size, total_records)
            chunk_data = all_businesses[start_idx:end_idx]
            
            # T·∫°o t√™n file v·ªõi s·ªë th·ª© t·ª±
            if timestamp:
                if '.' in output_file:
                    name_parts = output_file.rsplit('.', 1)
                    chunk_filename = f"{timestamp}_{name_parts[0]}_part{i+1:03d}.{name_parts[1]}"
                else:
                    chunk_filename = f"{timestamp}_{output_file}_part{i+1:03d}"
            else:
                if '.' in output_file:
                    name_parts = output_file.rsplit('.', 1)
                    chunk_filename = f"{name_parts[0]}_part{i+1:03d}.{name_parts[1]}"
                else:
                    chunk_filename = f"{output_file}_part{i+1:03d}"
            
            with open(chunk_filename, 'w', encoding='utf-8') as f:
                json.dump(chunk_data, f, ensure_ascii=False, indent=2)
            
            print(f"   ‚úì Part {i+1}/{num_files}: {chunk_filename} ({len(chunk_data)} records)")
        
        print(f"\n‚úÖ ƒê√£ chia v√† l∆∞u th√†nh {num_files} files")



# ===== C√°c h√†m helper ƒë·ªÉ nh·∫≠p query =====

def get_queries_from_args():
    """L·∫•y queries t·ª´ command line"""
    import sys
    if len(sys.argv) > 1:
        return sys.argv[1:]
    return None


def parse_cli_args(argv: List[str]):
    """Parse CLI args for save mode, special commands, file input, and queries."""
    save_mode = "per_query"
    special_command = None
    file_path = None
    queries: List[str] = []

    i = 0
    while i < len(argv):
        arg = argv[i]

        if arg == "--export":
            special_command = "export"
            i += 1
            continue
        if arg == "--status":
            special_command = "status"
            i += 1
            continue
        if arg == "--file" and i + 1 < len(argv):
            file_path = argv[i + 1]
            i += 2
            continue
        if arg == "--save-mode" and i + 1 < len(argv):
            save_mode = argv[i + 1].strip().lower()
            i += 2
            continue
        if arg == "--combined":
            save_mode = "combined"
            i += 1
            continue
        if arg.startswith("--"):
            i += 1
            continue

        queries.append(arg)
        i += 1

    if save_mode not in ("per_query", "combined"):
        print(f"‚ö†Ô∏è save-mode kh√¥ng h·ª£p l·ªá: {save_mode} (d√πng m·∫∑c ƒë·ªãnh per_query)")
        save_mode = "per_query"

    return save_mode, special_command, file_path, queries


def get_queries_from_file(file_path: str) -> List[str]:
    """ƒê·ªçc queries t·ª´ file"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip()]
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc file: {e}")
        return []


def get_queries_interactive() -> List[str]:
    """Nh·∫≠p queries interactive"""
    print("\nüìù NH·∫¨P C√ÅC QUERY T√åM KI·∫æM")
    print("\nüéØ Ch·ªçn ch·∫ø ƒë·ªô nh·∫≠p:")
    print("   1. Nh·∫≠p t·ª´ng query (m·ªói d√≤ng 1 query, Enter 2 l·∫ßn ƒë·ªÉ k·∫øt th√∫c)")
    print("   2. Paste t·∫•t c·∫£ queries c√πng l√∫c (Ctrl+D ho·∫∑c Ctrl+Z ƒë·ªÉ k·∫øt th√∫c)")
    print("=" * 60)
    
    mode = input("Ch·ªçn ch·∫ø ƒë·ªô (1/2, Enter = 1): ").strip() or "1"
    
    if mode == "2":
        # Ch·∫ø ƒë·ªô paste nhi·ªÅu queries
        print("\nüìã Paste t·∫•t c·∫£ queries v√†o ƒë√¢y (m·ªói d√≤ng 1 query)")
        print("   Nh·∫•n Ctrl+D (Linux/Mac) ho·∫∑c Ctrl+Z + Enter (Windows) ƒë·ªÉ k·∫øt th√∫c\n")
        
        queries = []
        try:
            while True:
                line = input()
                if line.strip():
                    queries.append(line.strip())
        except EOFError:
            # Ctrl+D ho·∫∑c Ctrl+Z
            pass
        
        if queries:
            print(f"\n‚úÖ ƒê√£ nh·∫≠n {len(queries)} queries:")
            for i, q in enumerate(queries, 1):
                print(f"   {i}. {q}")
        
        return queries
    
    else:
        # Ch·∫ø ƒë·ªô nh·∫≠p t·ª´ng query
        print("\nüìù Nh·∫≠p t·ª´ng query, m·ªói query 1 d√≤ng")
        print("   Nh·∫•n Enter 2 l·∫ßn li√™n ti·∫øp ƒë·ªÉ k·∫øt th√∫c\n")
        
        queries = []
        empty_count = 0
        
        while True:
            query = input(f"Query {len(queries) + 1}: ").strip()
            
            if not query:
                empty_count += 1
                if empty_count >= 2:
                    break
                continue
            
            empty_count = 0
            queries.append(query)
            print(f"   ‚úì ƒê√£ th√™m: {query}")
        
        return queries


async def main():
    """H√†m ch√≠nh v·ªõi h·ªó tr·ª£ resume v√† graceful shutdown"""
    import sys
    global shutdown_requested, pause_requested, save_requested
    
    # ============== C·∫§U H√åNH (Conservative - An to√†n) ==============
    HEADLESS = False  # False = hi·ªán browser ƒë·ªÉ xem process
    DELAY_BETWEEN_SEARCHES = 8  # Delay gi·ªØa c√°c query
    CONCURRENT_TABS = 3  # S·ªë tabs song song
    BATCH_SAVE_INTERVAL = 5  # L∆∞u state sau m·ªói 5 items
    # ===============================================================
    
    # Setup signal handlers for graceful shutdown
    def signal_handler(signum: int, frame) -> None:
        global shutdown_requested
        print("\n\nüõë ƒêang d·ª´ng crawl... L∆∞u d·ªØ li·ªáu hi·ªán t·∫°i...")
        shutdown_requested = True
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    print("=" * 70)
    print("üó∫Ô∏è  GOOGLE MAPS BUSINESS SCRAPER")
    print("   üìå Features: Resume t·ª´ v·ªã tr√≠ d·ª´ng | Graceful shutdown | Excel export")
    print("=" * 70)
    
    save_mode, special_command, file_path, queries_from_args = parse_cli_args(sys.argv[1:])

    # Check for special commands
    if special_command == "export":
        print("üìä Exporting Excel from saved state files...")
        export_from_state_files()
        return

    if special_command == "status":
        state_files = list_saved_states()
        if state_files:
            print(f"\nüìÇ Saved states ({len(state_files)}):")
            for sf in state_files:
                filename = sf.stem.replace("_state", "")
                state = CrawlState.load(filename)
                if state:
                    status = "‚úÖ completed" if state.completed else f"‚è∏Ô∏è {state.current_index}/{len(state.urls)}"
                    print(f"   ‚Ä¢ {state.query}: {len(state.results)} results [{status}]")
        else:
            print("\nüìÇ Kh√¥ng c√≥ state files n√†o ƒë∆∞·ª£c l∆∞u")
        return
    
    # ===== NH·∫¨P QUERIES =====
    queries = None

    # C√°ch 1: Command line (positional)
    if queries_from_args:
        queries = queries_from_args
        print(f"\n‚úÖ ƒê√£ nh·∫≠n {len(queries)} queries t·ª´ command line\n")

    # C√°ch 2: T·ª´ file
    if not queries and file_path:
        queries = get_queries_from_file(file_path)
        if queries:
            print(f"\n‚úÖ ƒê√£ ƒë·ªçc {len(queries)} queries t·ª´ file: {file_path}\n")
    
    # C√°ch 3: Interactive
    if not queries:
        print("\nüí° H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:")
        print("   1. Command line: python script.py \"query 1\" \"query 2\"")
        print("   2. T·ª´ file: python script.py --file queries.txt")
        print("   3. Export Excel: python script.py --export")
        print("   4. Xem status: python script.py --status")
        print("   5. L∆∞u chung: python script.py --save-mode combined")
        print("   6. Interactive: nh·∫≠p tr·ª±c ti·∫øp\n")
        
        use_interactive = input("B·∫°n c√≥ mu·ªën nh·∫≠p queries ngay? (y/n): ").lower()
        if use_interactive == 'y':
            queries = get_queries_interactive()
    
    if not queries:
        print("\n‚ùå Kh√¥ng c√≥ query ƒë·ªÉ search!")
        return
    
    # Hi·ªÉn th·ªã queries
    print("\nüìã DANH S√ÅCH QUERIES:")
    for i, q in enumerate(queries, 1):
        print(f"   {i}. {q}")
    print()
    
    print(f"‚ö° Ch·∫ø ƒë·ªô: {CONCURRENT_TABS} tabs song song")
    print(f"‚è±Ô∏è  Delay: {DELAY_BETWEEN_SEARCHES}s gi·ªØa c√°c query")
    print(f"üíæ Auto-save: Sau m·ªói {BATCH_SAVE_INTERVAL} items")
    print(f"üíæ Ch·∫ø ƒë·ªô l∆∞u: {save_mode}")
    
    # Print keyboard controls
    print_controls_banner()
    
    # Initialize keyboard controller
    keyboard_controller = KeyboardController()
    
    # Kh·ªüi t·∫°o scraper
    scraper = GoogleMapsScraper(headless=HEADLESS, concurrent_tabs=CONCURRENT_TABS)
    
    # Start keyboard listener
    loop = asyncio.get_event_loop()
    keyboard_controller.start(loop)
    
    all_results_by_query: Dict[str, List[Dict[str, str]]] = {}

    # Process each query separately for better resume support
    for query_idx, query in enumerate(queries, 1):
        if shutdown_requested:
            print("\nüõë ƒê√£ d·ª´ng theo y√™u c·∫ßu ng∆∞·ªùi d√πng")
            break
        
        filename = sanitize_query_to_filename(query)
        print(f"\n{'='*60}")
        print(f"üîç [{query_idx}/{len(queries)}] Query: {query}")
        print(f"   üìÅ Filename: {filename}")
        print(f"{'='*60}")
        
        # Check for existing state
        existing_state = CrawlState.find_existing(query)
        state: CrawlState
        
        if existing_state and not existing_state.completed:
            print(f"\nüì• T√¨m th·∫•y state tr∆∞·ªõc ƒë√≥:")
            print(f"   ‚Ä¢ ƒê√£ crawl: {len(existing_state.results)} k·∫øt qu·∫£")
            print(f"   ‚Ä¢ V·ªã tr√≠: {existing_state.current_index}/{len(existing_state.urls)}")
            print(f"   ‚Ä¢ C·∫≠p nh·∫≠t: {existing_state.last_updated}")
            
            resume_choice = input("\n   Ti·∫øp t·ª•c t·ª´ v·ªã tr√≠ d·ª´ng? (y/n, Enter=y): ").lower().strip()
            if resume_choice in ['', 'y', 'yes']:
                state = existing_state
                print(f"   ‚úÖ Ti·∫øp t·ª•c t·ª´ index {state.current_index}")
            else:
                print("   üîÑ B·∫Øt ƒë·∫ßu l·∫°i t·ª´ ƒë·∫ßu")
                state = CrawlState(query=query, filename=filename)
        else:
            state = CrawlState(query=query, filename=filename)
        
        # Run the crawl
        async with async_playwright() as p:
            print("\nüåê ƒêang kh·ªüi ƒë·ªông browser...")
            
            browser = await p.chromium.launch(
                headless=HEADLESS,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                ]
            )
            
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080},
                locale="vi-VN",
                timezone_id="Asia/Ho_Chi_Minh",
            )
            
            page = await context.new_page()
            
            try:
                # If we don't have URLs yet, search for them
                if not state.urls:
                    print("   üó∫Ô∏è  ƒêang t√¨m ki·∫øm tr√™n Google Maps...")
                    from urllib.parse import quote_plus
                    
                    encoded_query = quote_plus(query)
                    maps_url = f"https://www.google.com/maps/search/{encoded_query}"
                    
                    await page.goto(maps_url, wait_until="domcontentloaded", timeout=60000)
                    
                    try:
                        await page.wait_for_selector('div[role="feed"]', timeout=10000)
                    except:
                        print("   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y danh s√°ch k·∫øt qu·∫£")
                        continue
                    
                    # Scroll to load all results
                    await scraper._scroll_to_load_all(page)
                    
                    # Get all URLs
                    items = await page.query_selector_all('a.hfpxzc')
                    if not items:
                        items = await page.query_selector_all('a[href*="/maps/place/"]')
                    
                    urls = []
                    for item in items:
                        href = await item.get_attribute('href')
                        if href and '/maps/place/' in href:
                            urls.append(href)
                    
                    urls = list(dict.fromkeys(urls))  # Remove duplicates
                    state.urls = urls
                    state.save()
                    
                    print(f"   üìä T√¨m th·∫•y {len(urls)} ƒë·ªãa ƒëi·ªÉm")
                
                # Process URLs from current_index
                total_urls = len(state.urls)
                start_index = state.current_index
                
                print(f"\n   üìù ƒêang crawl t·ª´ index {start_index + 1}/{total_urls}...")
                
                for idx in range(start_index, total_urls):
                    # Check for pause
                    while pause_requested and not shutdown_requested:
                        await asyncio.sleep(0.5)
                    
                    if shutdown_requested:
                        print("\n   üõë ƒêang l∆∞u state v√† tho√°t...")
                        state.save()
                        break
                    
                    # Check for manual save request
                    if save_requested:
                        state.save()
                        print(f"\n   üíæ Manual save: {len(state.results)} k·∫øt qu·∫£")
                        save_requested = False
                    
                    url = state.urls[idx]
                    
                    # Extract business info
                    result = await scraper._extract_from_url(url, context, idx + 1, total_urls)
                    
                    if result and result.get('name'):
                        state.results.append(result)
                    
                    state.current_index = idx + 1
                    
                    # Save state periodically
                    if (idx + 1) % BATCH_SAVE_INTERVAL == 0:
                        state.save()
                        print(f"\n   üíæ ƒê√£ l∆∞u state ({len(state.results)} k·∫øt qu·∫£)")
                    
                    # Small delay
                    await asyncio.sleep(0.5 + random.uniform(0, 0.3))
                
                # Mark completed if finished all URLs
                if state.current_index >= total_urls and not shutdown_requested:
                    state.mark_completed()
                    print(f"\n   ‚úÖ Ho√†n th√†nh query: {len(state.results)} k·∫øt qu·∫£")
                
            except Exception as e:
                print(f"\n   ‚ùå L·ªói: {type(e).__name__}: {e}")
                state.save()  # Save on error
                
            finally:
                await browser.close()
        
        # Track results by query for combined export
        if state.results:
            all_results_by_query[query] = state.results

        # Save to Excel per query (if configured)
        if save_mode == "per_query":
            if state.results:
                print(f"\n   üìä Exporting {len(state.results)} results to Excel...")
                excel_path = save_to_excel(state.results, query)
                if excel_path:
                    print(f"   ‚úÖ Excel exported: {excel_path}")
                    if state.completed:
                        state.delete_state_file()
            else:
                print("\n   ‚ö†Ô∏è Kh√¥ng c√≥ k·∫øt qu·∫£ ƒë·ªÉ export")
        
        # Delay before next query
        if query_idx < len(queries) and not shutdown_requested:
            delay_time = DELAY_BETWEEN_SEARCHES + random.uniform(0, 2)
            print(f"\n   ‚è≥ Ch·ªù {delay_time:.1f}s tr∆∞·ªõc query ti·∫øp theo...")
            await asyncio.sleep(delay_time)
    
    # Stop keyboard listener
    keyboard_controller.stop()
    
    # Combined export (if configured)
    if save_mode == "combined" and all_results_by_query:
        print(f"\nüìä Exporting combined results from {len(all_results_by_query)} queries...")
        combined_path = save_combined_excel(all_results_by_query)
        if combined_path:
            print(f"‚úÖ Combined Excel exported: {combined_path}")

    print("\n" + "=" * 70)
    if shutdown_requested:
        print("üõë ƒê√É D·ª™NG - D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u")
        print("   üí° Ch·∫°y l·∫°i script ƒë·ªÉ ti·∫øp t·ª•c t·ª´ v·ªã tr√≠ d·ª´ng")
    else:
        print("‚úÖ HO√ÄN TH√ÄNH!")
    print(f"üìä ƒê√£ x·ª≠ l√Ω {len(queries)} queries")
    print(f"üìÅ K·∫øt qu·∫£: th∆∞ m·ª•c {OUTPUT_DIR}/")
    print("=" * 70)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\n" + "=" * 70)
        print("üõë INTERRUPTED BY USER (Ctrl+C)")
        print("=" * 70)
        
        # Export Excel from saved states
        state_files = list_saved_states()
        if state_files:
            print(f"\nüìÇ ƒêang export {len(state_files)} state files th√†nh Excel...")
            for state_file in state_files:
                filename = state_file.stem.replace("_state", "")
                state = CrawlState.load(filename)
                if state and state.results:
                    print(f"\n   üìä {state.query}: {len(state.results)} results")
                    excel_path = save_to_excel(state.results, state.query)
                    if excel_path:
                        print(f"   ‚úÖ Saved: {excel_path}")
        
        print("\nüí° Ch·∫°y l·∫°i script ƒë·ªÉ ti·∫øp t·ª•c t·ª´ v·ªã tr√≠ d·ª´ng")
        print("=" * 70)
