"""
Google Maps Business Scraper
T·ª± ƒë·ªông t√¨m ki·∫øm v√† thu th·∫≠p th√¥ng tin doanh nghi·ªáp t·ª´ Google Maps
"""

import json
import asyncio
import re
from datetime import datetime
from typing import List, Dict, Optional
from playwright.async_api import async_playwright, Page, BrowserContext, TimeoutError as PlaywrightTimeoutError


class GoogleMapsScraper:
    """Scraper Google Maps s·ª≠ d·ª•ng Playwright"""
    
    def __init__(self, headless: bool = False, concurrent_tabs: int = 5):
        self.headless = headless
        self.concurrent_tabs = concurrent_tabs
        self.max_scroll_attempts = 10  # S·ªë l·∫ßn scroll t·ªëi ƒëa ƒë·ªÉ load h·∫øt k·∫øt qu·∫£
    
    async def search_google_maps(self, query: str, page: Page, context: BrowserContext) -> List[Dict]:
        """
        T√¨m ki·∫øm tr√™n Google Maps v√† l·∫•y danh s√°ch k·∫øt qu·∫£
        
        Args:
            query: T·ª´ kh√≥a t√¨m ki·∫øm
            page: Playwright page instance
            context: Browser context for multi-tab processing
            
        Returns:
            List c√°c k·∫øt qu·∫£ business
        """
        from urllib.parse import quote_plus
        
        encoded_query = quote_plus(query)
        maps_url = f"https://www.google.com/maps/search/{encoded_query}"
        
        try:
            print(f"   üó∫Ô∏è  ƒêang truy c·∫≠p Google Maps...")
            await page.goto(maps_url, wait_until="domcontentloaded", timeout=60000)
            
            # ƒê·ª£i k·∫øt qu·∫£ load v·ªõi smart wait
            print(f"   ‚è≥ ƒêang ch·ªù k·∫øt qu·∫£ Maps load...")
            try:
                await page.wait_for_selector('div[role="feed"]', timeout=10000)
                print(f"   ‚úÖ ƒê√£ load ƒë∆∞·ª£c danh s√°ch k·∫øt qu·∫£")
            except:
                print(f"   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y danh s√°ch k·∫øt qu·∫£")
                return []
            
            # Scroll ƒë·ªÉ load t·∫•t c·∫£ k·∫øt qu·∫£
            results_count = await self._scroll_to_load_all(page)
            print(f"   üìä T·ªïng s·ªë k·∫øt qu·∫£ sau khi scroll: {results_count}")
            
            # Parse t·∫•t c·∫£ k·∫øt qu·∫£ v·ªõi multi-tab
            businesses = await self._parse_all_results_with_tabs(page, context)
            
            return businesses
            
        except PlaywrightTimeoutError:
            print(f"   ‚è±Ô∏è Timeout khi load Google Maps")
            return []
        except Exception as e:
            print(f"   ‚ùå L·ªói: {type(e).__name__}: {e}")
            return []
    
    async def _scroll_to_load_all(self, page: Page) -> int:
        """
        Scroll sidebar ƒë·ªÉ load to√†n b·ªô k·∫øt qu·∫£
        
        Returns:
            S·ªë l∆∞·ª£ng k·∫øt qu·∫£ hi·ªán t·∫°i
        """
        print(f"   üîÑ ƒêang scroll ƒë·ªÉ load th√™m k·∫øt qu·∫£...")
        
        # Selector cho scrollable container
        # Google Maps c√≥ th·ªÉ thay ƒë·ªïi, th·ª≠ nhi·ªÅu selector
        scrollable_selectors = [
            'div[role="feed"]',
            'div.m6QErb',  # Class name c√≥ th·ªÉ thay ƒë·ªïi
            '[aria-label*="Results"]',
        ]
        
        scrollable_elem = None
        for selector in scrollable_selectors:
            elem = await page.query_selector(selector)
            if elem:
                scrollable_elem = elem
                print(f"      ‚úì T√¨m th·∫•y scrollable container: {selector}")
                break
        
        if not scrollable_elem:
            print(f"      ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y scrollable container")
            return 0
        
        try:
            previous_count = 0
            no_change_attempts = 0
            
            for i in range(self.max_scroll_attempts):
                # Scroll xu·ªëng
                await page.evaluate('''
                    const scrollable = document.querySelector('div[role="feed"]');
                    if (scrollable) {
                        scrollable.scrollBy(0, scrollable.scrollHeight);
                    }
                ''')
                
                # ƒê·ª£i load - gi·∫£m t·ª´ 3s xu·ªëng 1.5s
                await asyncio.sleep(1.5)
                
                # ƒê·∫øm s·ªë item
                # Th·ª≠ nhi·ªÅu selector
                items = []
                for sel in ['a[href*="/maps/place/"]', 'div[role="article"]', 'a.hfpxzc']:
                    items = await page.query_selector_all(sel)
                    if items:
                        break
                
                current_count = len(items)
                
                if current_count > previous_count:
                    print(f"      ‚îú‚îÄ Scroll {i+1}: {current_count} k·∫øt qu·∫£ (+{current_count - previous_count})")
                    previous_count = current_count
                    no_change_attempts = 0
                else:
                    no_change_attempts += 1
                    print(f"      ‚îú‚îÄ Scroll {i+1}: {current_count} k·∫øt qu·∫£ (kh√¥ng tƒÉng)")
                    
                    if no_change_attempts >= 3:
                        print(f"      ‚îî‚îÄ ƒê√£ load h·∫øt (kh√¥ng tƒÉng sau 3 l·∫ßn)")
                        break
            
            return previous_count
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è L·ªói scroll: {e}")
            return 0
    
    async def _parse_all_results_with_tabs(self, page: Page, context: BrowserContext) -> List[Dict]:
        """
        Parse t·∫•t c·∫£ k·∫øt qu·∫£ s·ª≠ d·ª•ng multi-tab parallel processing
        
        Returns:
            List c√°c business info
        """
        businesses = []
        
        try:
            # Thu th·∫≠p t·∫•t c·∫£ URLs t·ª´ search results
            possible_selectors = [
                'a.hfpxzc',  # Link ch√≠nh c·ªßa m·ªói business (ph·ªï bi·∫øn nh·∫•t)
                'a[href*="/maps/place/"]',  # Fallback
            ]
            
            urls = []
            used_selector = None
            
            for selector in possible_selectors:
                items = await page.query_selector_all(selector)
                if items and len(items) > 0:
                    used_selector = selector
                    print(f"   ‚úÖ T√¨m th·∫•y {len(items)} items v·ªõi selector: {selector}")
                    
                    # Extract URLs
                    for item in items:
                        href = await item.get_attribute('href')
                        if href and '/maps/place/' in href:
                            urls.append(href)
                    break
            
            if not urls:
                print(f"   ‚ùå Kh√¥ng t√¨m th·∫•y business URLs!")
                # Debug: l∆∞u HTML v√† screenshot
                html_content = await page.content()
                with open('debug_maps.html', 'w', encoding='utf-8') as f:
                    f.write(html_content)
                await page.screenshot(path='debug_maps.png')
                print(f"   üíæ ƒê√£ l∆∞u debug_maps.html v√† debug_maps.png")
                return businesses
            
            # Lo·∫°i b·ªè duplicates
            urls = list(dict.fromkeys(urls))
            
            # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng
            max_items = min(len(urls), 30)
            urls = urls[:max_items]
            
            print(f"   üìù S·∫Ω crawl {len(urls)} businesses v·ªõi {self.concurrent_tabs} tabs song song")
            print(f"   üí° Multi-tab parallel processing...\n")
            
            # Process URLs in batches
            batch_size = self.concurrent_tabs
            total_processed = 0
            
            for batch_idx in range(0, len(urls), batch_size):
                batch_urls = urls[batch_idx:batch_idx + batch_size]
                batch_num = (batch_idx // batch_size) + 1
                total_batches = (len(urls) + batch_size - 1) // batch_size
                
                print(f"   üîÑ Batch {batch_num}/{total_batches}: Processing {len(batch_urls)} items in parallel...")
                
                # Process batch in parallel
                tasks = [
                    self._extract_from_url(url, context, total_processed + i + 1, max_items)
                    for i, url in enumerate(batch_urls)
                ]
                
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # Collect successful results
                for result in batch_results:
                    if isinstance(result, dict) and result.get('name'):
                        businesses.append(result)
                    elif isinstance(result, Exception):
                        print(f"      ‚ö†Ô∏è Error in batch: {result}")
                
                total_processed += len(batch_urls)
                print()
            
            print(f"   ‚úÖ ƒê√£ parse th√†nh c√¥ng {len(businesses)}/{max_items} k·∫øt qu·∫£")
            return businesses
            
        except Exception as e:
            print(f"   ‚ùå L·ªói khi parse: {e}")
            import traceback
            traceback.print_exc()
            return businesses
    
    async def _extract_from_url(self, url: str, context: BrowserContext, index: int, total: int) -> Optional[Dict]:
        """
        M·ªü URL trong tab m·ªõi v√† extract business info
        
        Args:
            url: Business detail URL
            context: Browser context
            index: Current index for logging
            total: Total items for logging
            
        Returns:
            Business info dict ho·∫∑c None
        """
        page = None
        try:
            # M·ªü tab m·ªõi
            page = await context.new_page()
            
            # Stagger tab opening ƒë·ªÉ tr√°nh b·ªã detect (50ms delay)
            await asyncio.sleep(0.05 * (index % 5))
            
            # Navigate v·ªõi timeout ƒë·ªß d√†i
            await page.goto(url, wait_until="domcontentloaded", timeout=45000)
            
            # Thay v√¨ wait networkidle, wait cho selector quan tr·ªçng
            try:
                # Wait cho t√™n business xu·∫•t hi·ªán
                await page.wait_for_selector('h1', timeout=8000)
            except:
                # N·∫øu kh√¥ng c√≥ h1, v·∫´n th·ª≠ extract
                pass
            
            # Th√™m delay nh·ªè ƒë·ªÉ panel load ƒë·∫ßy ƒë·ªß
            await asyncio.sleep(1)
            
            # Extract info
            business_info = await self._extract_from_detail_panel(page)
            
            if business_info and business_info.get('name'):
                print(f"      ‚úì [{index}/{total}] {business_info['name'][:50]}")
                if business_info.get('phone'):
                    print(f"          üìû {business_info['phone']}")
            else:
                print(f"      ‚ö†Ô∏è [{index}/{total}] Kh√¥ng l·∫•y ƒë∆∞·ª£c th√¥ng tin")
            
            return business_info
            
        except Exception as e:
            print(f"      ‚ùå [{index}/{total}] L·ªói: {type(e).__name__}: {str(e)[:50]}")
            return None
        finally:
            if page:
                await page.close()
    
    async def _extract_from_detail_panel(self, page: Page) -> Optional[Dict]:
        """
        Extract th√¥ng tin t·ª´ detail panel b√™n ph·∫£i
        (Sau khi ƒë√£ click v√†o m·ªôt business)
        """
        try:
            
            # L·∫•y t√™n - nhi·ªÅu selector kh√°c nhau
            name = None
            name_selectors = [
                'h1.DUwDvf',  # Selector ph·ªï bi·∫øn nh·∫•t
                'h1.fontHeadlineLarge',
                'h1',
                'div.fontHeadlineLarge span',
                '[role="main"] h1',
            ]
            
            for selector in name_selectors:
                name_elem = await page.query_selector(selector)
                if name_elem:
                    name_text = await name_elem.inner_text()
                    name_text = name_text.strip()
                    if name_text and len(name_text) > 2:
                        name = name_text
                        break
            
            if not name:
                return None
            
            # L·∫•y s·ªë ƒëi·ªán tho·∫°i - nhi·ªÅu c√°ch
            phone = None
            
            # C√°ch 1: T√¨m button c√≥ data-item-id ch·ª©a "phone"
            phone_button = await page.query_selector('button[data-item-id*="phone"]')
            if phone_button:
                aria_label = await phone_button.get_attribute('aria-label') or ''
                phone = self._extract_phone(aria_label)
            
            # C√°ch 2: T√¨m link tel:
            if not phone:
                tel_link = await page.query_selector('a[href^="tel:"]')
                if tel_link:
                    href = await tel_link.get_attribute('href') or ''
                    phone = self._extract_phone(href)
            
            # C√°ch 3: T√¨m trong aria-label c√≥ "Phone"
            if not phone:
                phone_buttons = await page.query_selector_all('button[aria-label*="Phone"], button[aria-label*="ƒêi·ªán tho·∫°i"]')
                for btn in phone_buttons:
                    aria_label = await btn.get_attribute('aria-label') or ''
                    phone = self._extract_phone(aria_label)
                    if phone:
                        break
            
            # C√°ch 4: T√¨m trong to√†n b·ªô panel text
            if not phone:
                # L·∫•y text t·ª´ ph·∫ßn th√¥ng tin chi ti·∫øt
                detail_sections = await page.query_selector_all('div.rogA2c')  # Sections ch·ª©a info
                for section in detail_sections:
                    text = await section.inner_text()
                    phone = self._extract_phone(text)
                    if phone:
                        break
            
            # L·∫•y ƒë·ªãa ch·ªâ
            address = "Ch∆∞a c√≥ th√¥ng tin"
            
            # C√°ch 1: T·ª´ button address
            addr_button = await page.query_selector('button[data-item-id*="address"]')
            if addr_button:
                aria_label = await addr_button.get_attribute('aria-label') or ''
                if 'Address:' in aria_label or 'ƒê·ªãa ch·ªâ:' in aria_label:
                    parts = aria_label.replace('Address:', '|').replace('ƒê·ªãa ch·ªâ:', '|').split('|')
                    if len(parts) > 1:
                        address = parts[1].strip()
            
            # C√°ch 2: T√¨m trong div ch·ª©a ƒë·ªãa ch·ªâ (th∆∞·ªùng c√≥ class fontBodyMedium)
            if address == "Ch∆∞a c√≥ th√¥ng tin":
                addr_divs = await page.query_selector_all('div.fontBodyMedium')
                for div in addr_divs:
                    text = await div.inner_text()
                    text = text.strip()
                    # ƒê·ªãa ch·ªâ th∆∞·ªùng c√≥ t√™n th√†nh ph·ªë v√† d√†i h∆°n
                    if any(city in text for city in ['H√† N·ªôi', 'TP.HCM', 'ƒê√† N·∫µng', 'C·∫ßn Th∆°', 'H·∫£i Ph√≤ng', 'Vi·ªát Nam']):
                        if len(text) > 15 and not any(x in text for x in ['‚òÖ', 'ƒë√°nh gi√°', 'rating', 'M·ªü c·ª≠a', 'ƒê√≥ng c·ª≠a']):
                            address = text
                            break
            
            # C√°ch 3: Fallback - t√¨m trong to√†n b·ªô panel
            if address == "Ch∆∞a c√≥ th√¥ng tin":
                panel_elem = await page.query_selector('[role="main"]')
                if panel_elem:
                    panel_text = await panel_elem.inner_text()
                    address = self._extract_address_from_text(panel_text)
            
            # L·∫•y website
            website = None
            
            # C√°ch 1: T√¨m button c√≥ data-item-id ch·ª©a "authority" ho·∫∑c "website"
            website_button = await page.query_selector('button[data-item-id*="authority"], button[data-item-id*="website"]')
            if website_button:
                aria_label = await website_button.get_attribute('aria-label') or ''
                # Extract URL t·ª´ aria-label
                website = self._extract_website(aria_label)
            
            # C√°ch 2: T√¨m link c√≥ href b·∫Øt ƒë·∫ßu b·∫±ng http
            if not website:
                # T√¨m trong panel ch√≠nh, tr√°nh c√°c link internal c·ªßa Google Maps
                panel_elem = await page.query_selector('[role="main"]')
                if panel_elem:
                    website_links = await panel_elem.query_selector_all('a[href^="http"]')
                    for link in website_links:
                        href = await link.get_attribute('href') or ''
                        # Lo·∫°i b·ªè c√°c link c·ªßa Google
                        if 'google.com' not in href and 'gstatic.com' not in href:
                            website = href
                            break
            
            # L·∫•y th·ªùi gian ho·∫°t ƒë·ªông (opening hours)
            opening_hours = None
            
            # C√°ch 1: T√¨m button c√≥ data-item-id ch·ª©a "hours"
            hours_button = await page.query_selector('button[data-item-id*="hours"]')
            if hours_button:
                aria_label = await hours_button.get_attribute('aria-label') or ''
                opening_hours = self._extract_opening_hours(aria_label)
            
            # C√°ch 2: T√¨m trong c√°c div ch·ª©a th√¥ng tin gi·ªù m·ªü c·ª≠a
            if not opening_hours:
                # T√¨m text c√≥ ch·ª©a "Open", "Closes", "M·ªü c·ª≠a", "ƒê√≥ng c·ª≠a"
                hours_indicators = ['Open', 'Closes', 'Opens', 'M·ªü c·ª≠a', 'ƒê√≥ng c·ª≠a', '24 hours', '24 gi·ªù']
                all_divs = await page.query_selector_all('div.fontBodyMedium, div.fontBodySmall')
                for div in all_divs:
                    text = await div.inner_text()
                    text = text.strip()
                    if any(indicator in text for indicator in hours_indicators):
                        # T√¨m th√™m context xung quanh ƒë·ªÉ l·∫•y ƒë·∫ßy ƒë·ªß th√¥ng tin
                        parent = await div.evaluate_handle('el => el.parentElement')
                        if parent:
                            hours_text = await parent.as_element().inner_text()
                            hours_text = hours_text.strip()
                            if len(hours_text) > 3:
                                opening_hours = hours_text
                                break
            
            return {
                "name": name,
                "phone": phone,
                "address": address,
                "website": website,
                "opening_hours": opening_hours,
            }
            
        except Exception as e:
            print(f"         L·ªói extract detail: {e}")
            return None
    
    def _extract_address_from_text(self, text: str) -> str:
        """Extract ƒë·ªãa ch·ªâ t·ª´ m·ªôt ƒëo·∫°n text d√†i"""
        lines = text.split('\n')
        cities = ['H√† N·ªôi', 'TP.HCM', 'TP HCM', 'S√†i G√≤n', 'ƒê√† N·∫µng', 'C·∫ßn Th∆°', 'H·∫£i Ph√≤ng', 'Vi·ªát Nam']
        
        for line in lines:
            line = line.strip()
            # T√¨m d√≤ng ch·ª©a t√™n th√†nh ph·ªë v√† ƒë·ªß d√†i
            for city in cities:
                if city in line and len(line) > 15:
                    # Lo·∫°i b·ªè c√°c prefix kh√¥ng c·∫ßn thi·∫øt
                    if ':' in line:
                        line = line.split(':', 1)[1].strip()
                    return line
        
        return "Ch∆∞a c√≥ th√¥ng tin"
    
    def _extract_website(self, text: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t website URL t·ª´ text"""
        # Pattern cho URL
        url_pattern = r'https?://[^\s\"\',<>]+'
        matches = re.findall(url_pattern, text)
        
        if matches:
            url = matches[0]
            # Lo·∫°i b·ªè c√°c URL c·ªßa Google
            if 'google.com' not in url and 'gstatic.com' not in url:
                return url
        
        # N·∫øu kh√¥ng t√¨m th·∫•y http://, th·ª≠ t√¨m domain pattern
        domain_pattern = r'(?:www\.)?[a-zA-Z0-9-]+\.[a-zA-Z]{2,}(?:\.[a-zA-Z]{2,})?'
        domain_matches = re.findall(domain_pattern, text)
        
        if domain_matches:
            domain = domain_matches[0]
            # Th√™m https:// n·∫øu ch∆∞a c√≥
            if not domain.startswith(('http://', 'https://')):
                return f'https://{domain}'
            return domain
        
        return None
    
    def _extract_opening_hours(self, text: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t th√¥ng tin gi·ªù m·ªü c·ª≠a t·ª´ text"""
        # L√†m s·∫°ch aria-label
        # Th∆∞·ªùng c√≥ format: "Hours: Open ‚ãÖ Closes 5 PM" ho·∫∑c "Gi·ªù: M·ªü c·ª≠a ‚ãÖ ƒê√≥ng c·ª≠a 17:00"
        
        # Lo·∫°i b·ªè c√°c prefix nh∆∞ "Hours:", "Gi·ªù:", etc.
        cleaned = text
        for prefix in ['Hours:', 'Gi·ªù:', 'Opening hours:', 'Th·ªùi gian m·ªü c·ª≠a:']:
            if prefix in cleaned:
                cleaned = cleaned.split(prefix, 1)[1].strip()
        
        # N·∫øu c√≥ n·ªôi dung h·ª£p l·ªá
        if len(cleaned) > 3:
            # L√†m s·∫°ch th√™m c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát
            cleaned = cleaned.replace('‚ãÖ', '‚Ä¢').strip()
            return cleaned
        
        return None
    
    def _extract_phone(self, text: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t s·ªë ƒëi·ªán tho·∫°i t·ª´ text"""
        # Pattern cho s·ªë ƒëi·ªán tho·∫°i Vi·ªát Nam
        patterns = [
            r'(?:\+84|84|0)[\s.-]?\d{1,4}[\s.-]?\d{3}[\s.-]?\d{3,4}',
            r'(?:\+84|84|0)\d{9,10}',
            r'\b\d{10,11}\b',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            if matches:
                # L√†m s·∫°ch
                phone = re.sub(r'[^\d+]', '', matches[0])
                
                # Chu·∫©n h√≥a
                if phone.startswith('+84'):
                    phone = '0' + phone[3:]
                elif phone.startswith('84'):
                    phone = '0' + phone[2:]
                
                if 10 <= len(phone) <= 11:
                    return phone
        
        return None
    
    async def run_searches(self, queries: List[str], delay: float = 3.0) -> Dict[str, List[Dict]]:
        """
        Ch·∫°y nhi·ªÅu query search tr√™n Maps
        
        Args:
            queries: Danh s√°ch query
            delay: Delay gi·ªØa c√°c query
            
        Returns:
            Dict v·ªõi key l√† query, value l√† list k·∫øt qu·∫£
        """
        all_results = {}
        
        async with async_playwright() as p:
            print("üåê ƒêang kh·ªüi ƒë·ªông browser...")
            
            # Launch v·ªõi args t∆∞∆°ng t·ª± batdongsan_final.py
            browser = await p.chromium.launch(
                headless=self.headless,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                ]
            )
            
            # Context options
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080},
                locale="vi-VN",
                timezone_id="Asia/Ho_Chi_Minh",
            )
            
            page = await context.new_page()
            
            try:
                for i, query in enumerate(queries, 1):
                    print(f"\nüîç [{i}/{len(queries)}] ƒêang search: {query}")
                    
                    # Search tr√™n Maps v·ªõi context for multi-tab
                    businesses = await self.search_google_maps(query, page, context)
                    
                    all_results[query] = businesses
                    print(f"   ‚úÖ T·ªïng c·ªông: {len(businesses)} k·∫øt qu·∫£\n")
                    
                    # Delay
                    if i < len(queries):
                        print(f"   ‚è≥ Ch·ªù {delay}s tr∆∞·ªõc query ti·∫øp theo...")
                        await asyncio.sleep(delay)
            
            finally:
                await browser.close()
        
        return all_results


def save_results(results: Dict[str, List[Dict]], output_file: str, timestamp: str = ""):
    """L∆∞u k·∫øt qu·∫£ v√†o JSON file v·ªõi timestamp prefix
    
    Args:
        results: K·∫øt qu·∫£ scraping
        output_file: T√™n file g·ªëc
        timestamp: Timestamp ƒë·ªÉ th√™m v√†o prefix (format: YYYYMMDD_HHMMSS)
    """
    # G·ªôp v√† lo·∫°i tr√πng
    all_businesses = []
    seen_names = set()
    
    for query, businesses in results.items():
        for business in businesses:
            name = business.get("name", "")
            
            # Lo·∫°i tr√πng theo t√™n
            if name and name not in seen_names:
                seen_names.add(name)
                all_businesses.append(business)
    
    # Th√™m timestamp v√†o t√™n file n·∫øu c√≥
    if timestamp:
        # T√°ch t√™n file v√† extension
        if '.' in output_file:
            name_parts = output_file.rsplit('.', 1)
            final_filename = f"{timestamp}_{name_parts[0]}.{name_parts[1]}"
        else:
            final_filename = f"{timestamp}_{output_file}"
    else:
        final_filename = output_file
    
    # L∆∞u file
    with open(final_filename, 'w', encoding='utf-8') as f:
        json.dump(all_businesses, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ ƒê√£ l∆∞u {len(all_businesses)} doanh nghi·ªáp v√†o {final_filename}")


# ===== C√°c h√†m helper ƒë·ªÉ nh·∫≠p query =====

def get_queries_from_args():
    """L·∫•y queries t·ª´ command line"""
    import sys
    if len(sys.argv) > 1:
        return sys.argv[1:]
    return None


def get_queries_from_file(file_path: str) -> List[str]:
    """ƒê·ªçc queries t·ª´ file"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip()]
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc file: {e}")
        return []


def get_queries_interactive() -> List[str]:
    """Nh·∫≠p queries interactive"""
    print("\nüìù NH·∫¨P C√ÅC QUERY T√åM KI·∫æM")
    print("\nüéØ Ch·ªçn ch·∫ø ƒë·ªô nh·∫≠p:")
    print("   1. Nh·∫≠p t·ª´ng query (m·ªói d√≤ng 1 query, Enter 2 l·∫ßn ƒë·ªÉ k·∫øt th√∫c)")
    print("   2. Paste t·∫•t c·∫£ queries c√πng l√∫c (Ctrl+D ho·∫∑c Ctrl+Z ƒë·ªÉ k·∫øt th√∫c)")
    print("=" * 60)
    
    mode = input("Ch·ªçn ch·∫ø ƒë·ªô (1/2, Enter = 1): ").strip() or "1"
    
    if mode == "2":
        # Ch·∫ø ƒë·ªô paste nhi·ªÅu queries
        print("\nüìã Paste t·∫•t c·∫£ queries v√†o ƒë√¢y (m·ªói d√≤ng 1 query)")
        print("   Nh·∫•n Ctrl+D (Linux/Mac) ho·∫∑c Ctrl+Z + Enter (Windows) ƒë·ªÉ k·∫øt th√∫c\n")
        
        queries = []
        try:
            while True:
                line = input()
                if line.strip():
                    queries.append(line.strip())
        except EOFError:
            # Ctrl+D ho·∫∑c Ctrl+Z
            pass
        
        if queries:
            print(f"\n‚úÖ ƒê√£ nh·∫≠n {len(queries)} queries:")
            for i, q in enumerate(queries, 1):
                print(f"   {i}. {q}")
        
        return queries
    
    else:
        # Ch·∫ø ƒë·ªô nh·∫≠p t·ª´ng query
        print("\nüìù Nh·∫≠p t·ª´ng query, m·ªói query 1 d√≤ng")
        print("   Nh·∫•n Enter 2 l·∫ßn li√™n ti·∫øp ƒë·ªÉ k·∫øt th√∫c\n")
        
        queries = []
        empty_count = 0
        
        while True:
            query = input(f"Query {len(queries) + 1}: ").strip()
            
            if not query:
                empty_count += 1
                if empty_count >= 2:
                    break
                continue
            
            empty_count = 0
            queries.append(query)
            print(f"   ‚úì ƒê√£ th√™m: {query}")
        
        return queries


async def main():
    """H√†m ch√≠nh"""
    import sys
    
    # ============== C·∫§U H√åNH ==============
    OUTPUT_FILE = "google_maps_results.json"
    HEADLESS = False  # False = hi·ªán browser ƒë·ªÉ xem process
    DELAY_BETWEEN_SEARCHES = 5  # Gi√¢y
    CONCURRENT_TABS = 5  # S·ªë tabs song song
    # =====================================
    
    print("=" * 70)
    print("üó∫Ô∏è  GOOGLE MAPS BUSINESS SCRAPER (ASYNC MULTI-TAB)")
    print("=" * 70)
    
    # ===== NH·∫¨P QUERIES =====
    queries = None
    
    # C√°ch 1: Command line
    queries = get_queries_from_args()
    if queries:
        print(f"\n‚úÖ ƒê√£ nh·∫≠n {len(queries)} queries t·ª´ command line\n")
    
    # C√°ch 2: T·ª´ file
    if not queries and len(sys.argv) == 3 and sys.argv[1] == "--file":
        queries = get_queries_from_file(sys.argv[2])
        if queries:
            print(f"\n‚úÖ ƒê√£ ƒë·ªçc {len(queries)} queries t·ª´ file: {sys.argv[2]}\n")
    
    # C√°ch 3: Interactive
    if not queries:
        print("\nüí° H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:")
        print("   1. Command line: python script.py \"query 1\" \"query 2\"")
        print("   2. T·ª´ file: python script.py --file queries.txt")
        print("   3. Interactive: nh·∫≠p tr·ª±c ti·∫øp\n")
        
        use_interactive = input("B·∫°n c√≥ mu·ªën nh·∫≠p queries ngay? (y/n): ").lower()
        if use_interactive == 'y':
            queries = get_queries_interactive()
    
    if not queries:
        print("\n‚ùå Kh√¥ng c√≥ query ƒë·ªÉ search!")
        return
    
    # Hi·ªÉn th·ªã queries
    print("\nüìã DANH S√ÅCH QUERIES:")
    for i, q in enumerate(queries, 1):
        print(f"   {i}. {q}")
    print()
    
    print(f"‚ö° Ch·∫ø ƒë·ªô: {CONCURRENT_TABS} tabs song song (async)")
    print()
    
    # Kh·ªüi t·∫°o scraper
    scraper = GoogleMapsScraper(headless=HEADLESS, concurrent_tabs=CONCURRENT_TABS)
    
    # Ch·∫°y searches
    results = await scraper.run_searches(queries, delay=DELAY_BETWEEN_SEARCHES)
    
    # T·∫°o timestamp khi ho√†n th√†nh
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # L∆∞u k·∫øt qu·∫£ v·ªõi timestamp
    save_results(results, OUTPUT_FILE, timestamp=timestamp)
    
    print("\n" + "=" * 70)
    print("‚úÖ HO√ÄN TH√ÄNH!")
    print(f"üìä ƒê√£ search {len(queries)} queries")
    print(f"üìÅ K·∫øt qu·∫£: {OUTPUT_FILE}")
    print("=" * 70)


if __name__ == "__main__":
    asyncio.run(main())
